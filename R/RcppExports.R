# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#'Multivariate Normal Density Function
#'
#' @description This function calculates densities for a mutlivariate normal distribution.
#' 
#' Code adapted by Kyle McEvoy from Rcpp Gallery code found at https://gallery.rcpp.org/articles/dmvnorm_arma/
#' downloaded in 2020.
#' 
#' Originally written in 2013 by Nino Hardt, Dicko Ahmadou, Benjamin Christoffersen.
#' Slight modifications made by Kyle McEvoy 2021.
#' This code falls under the Gnu GPL v3 license. 
#' Code modified to only take a single vector of means.
#' 
#' @param x numeric vector The vector for which you want the density to be calculated.
#' @param mean numeric vector The vector of means for the Multivariate Normal Distribution.
#' should be the same length as x.
#' @param sigma numeric matrix Should be a square positive semi-definite Covariance matrix for the desired
#' multivariate normal distribution. Each dimension should be equal to the length of x. 
#' @return numeric The value of the pdf of the given Multivariate Normal distribution at the specified x value.
#' 
dmvnrm_arma <- function(x, mean, sigma, logd = FALSE) {
    .Call(`_BayesMultiLogit_dmvnrm_arma`, x, mean, sigma, logd)
}

#' Multinomial Logistic Regression using Data Augmentation (Metropolis-Hastings)
#' 
#' This function implements a data augmentation method for
#' multinomial logisitic regression using MCMC. The data augmentation method is
#' outlined in a paper by Jared D. Fisher and Kyle R. McEvoy titled
#' "Bayesian Multinomial Logistic Regression for Numerous Categories",
#' currently a work in progress. The sampler uses a Random-Walk Metropolis
#' algorithm for choosing proposals and acceptances.
#' 
#'
#' @param Y An N by C numeric matrix where the ith row is a set of
#' indicators for observation i of N total observations giving which
#' of the C categories the observation is classified into.
#' @param X An N by P numeric matrix where N is the total number of
#' observations and P is the total number of predictor variables the ith row
#' gives the values of the predictor variables for the ith outcome observation.
#' The first column of X should be an intercept column of 1s.
#' Non-intercept X columns should be centered and scaled by their standard deviations for best results. 
#' @param n_sample positive integer giving the number of samples to draw as
#'  output after burn-in.
#' @param n_burn non-negative integer giving the number of samples of burn-in
#'  before the chain output is saved.
#' @param n_sigma_check non-negative integer gives the period for the number of
#' samples on which to automatically tune the step size of the random walk.
#' @param prior character with either values "flat" or "normal"
#' @param prior_mean a vector of length equal to P the number of predictors.
#' Giving the mean for the normal prior on the coefficient vector. Only use if
#' prior = "normal".
#' @param prior_var a P by P matrix giving the covariance matrix of the prior
#' coefficients. Note that covariance structure is assumed to be constant across
#' categories. Must be a positive semi-definite matrix. Only use with prior = 
#' "normal".
#' @param reference_cat Either NULL or an integer between 1 and C where C is
#' the total number of categories. If left NULL, coefficients for all
#' categories will be generated, leading to a loss of identifiability in the
#' coefficients. If an integer, all of the coefficents for that category will
#' be held constant at 0.
#' @param probs logical If TRUE, for each non-burn chain sample of coefficients
#' the categorical probabilities of each observation will be calculated and returned.
#' @param progress logical If TRUE, the function will report progress at each thousandth
#' iteration.
#' @return List object containing posterior_coef, the chain of coefficient
#' values as an P by C by n_sample array. If probs are TRUE, it will also contain
#' posterior_prob a N by C by n_sample array containing the calculated probabilities of the observations
#' being classified into each of the C categories.
#' @examples 
#' Y <- matrix(0, nrow = 150, ncol = 3)
#' Y <- sapply(c(1,2,3), function(x) Y[, x] <- as.numeric((as.numeric(iris$Species) == x) )) 
#' X <- scale(iris[ , 1:4])
#' X <- cbind(1, X)
#' out <- multilogit_C(Y, X, n_sample = 3000, n_burn = 1500, n_sigma_check = 20,
#'  step_size = 0.1, prior = "normal", reference_cat = 1)
#'  
#' out_2 <- multilogit_C(Y, X, n_sample = 2000, n_burn = 500, prior = "flat")
#' 
multilogit_C <- function(Y, X, n_sample = 1000L, n_burn = 200L, n_sigma_check = 20L, prior = "flat", step_size = 0.1, prior_mean = NULL, prior_var = NULL, reference_cat = NULL, probs = TRUE, progress = TRUE) {
    .Call(`_BayesMultiLogit_multilogit_C`, Y, X, n_sample, n_burn, n_sigma_check, prior, step_size, prior_mean, prior_var, reference_cat, probs, progress)
}

#' Multinomial Logistic Regression using Data Augmentation (Elliptical Slice Sampler)
#' 
#' @description This function implements a data augmentation method for
#' multinomial logisitic regression using MCMC. The data augmentation method is
#' outlined in a paper by Jared D. Fisher and Kyle R. McEvoy titled
#' "Bayesian Multinomial Logistic Regression for Numerous Categories",
#' currently a work in progress.
#' 
#' The sampler uses the elliptical slice sampling algorithm
#' from the Murray, Adams, Mackay (2010) paper Elliptical Slice Sampling in the Journal of
#' Machine Learning Research. This sampler requires a multivariate normal prior on the betas.
#'
#' @param Y An N by C numeric matrix where the ith row is a set of
#' indicators for observation i of N total observations giving which
#' of the C categories the observation is classified into.
#' @param X An N by P numeric matrix where N is the total number of
#' observations and P is the total number of predictor variables (including the intercept) the ith row
#' gives the values of the predictor variables for the ith outcome observation.
#' The first column of X should be an intercept column of 1s.
#' Non-intercept X columns should be centered and scaled by their standard deviations for best results. 
#' @param n_sample positive integer giving the number of samples to draw as
#'  output after burn-in.
#' @param n_burn non-negative integer giving the number of samples of burn-in
#'  before the chain output is saved.
#' @param prior_mean a vector of length equal to P the number of predictors.
#' Giving the mean for the normal prior on the coefficient vector.
#' @param prior_var a P by P matrix giving the covariance matrix of the prior
#' coefficients. Note that covariance structure is assumed to be constant across
#' categories. Must be a positive semi-definite matrix. Only use with prior = 
#' "normal".
#' @param reference_cat Either NULL or an integer between 1 and C where C is
#' the total number of categories. If left NULL, coefficients for all
#' categories will be generated, leading to a loss of identifiability in the
#' coefficients. If an integer, all of the coefficents for that category will
#' be held constant at 0.
#' @param probs logical If TRUE, for each non-burn chain sample of coefficients
#' the categorical probabilities of each observation will be calculated and returned.
#' @param progress logical If TRUE, the function will report progress at each thousandth
#' iteration.
#' @return List object containing posterior_coef, the chain of coefficient
#' values as an P by C by n_sample array. If probs are TRUE, it will also contain
#' posterior_prob a N by C by n_sample array containing the calculated probabilities of the observations
#' being classified into each of the C categories.
#' @examples 
#' Y <- matrix(0, nrow = 150, ncol = 3)
#' Y <- sapply(c(1,2,3), function(x) Y[, x] <- as.numeric((as.numeric(iris$Species) == x) )) 
#' X <- scale(iris[ , 1:4])
#' X <- cbind(1, X)
#' out <- multilogit_C_ESS(Y, X, n_sample = 3000, n_burn = 1500, reference_cat = 1)
#'  
#' out_2 <- multilogit_C_ESS(Y, X, n_sample = 2000, n_burn = 500,
#'  prior_var = diag(x = 1, nrow = 5))
#' 
multilogit_C_ESS <- function(Y, X, n_sample = 1000L, n_burn = 200L, prior_mean = NULL, prior_var = NULL, reference_cat = NULL, probs = TRUE, progress = TRUE) {
    .Call(`_BayesMultiLogit_multilogit_C_ESS`, Y, X, n_sample, n_burn, prior_mean, prior_var, reference_cat, probs, progress)
}

#' Multinomial Logistic Regression using the Holmes-Held Method
#' 
#' This function implements the Holmes-Held method for
#' multinomial logistic regression described in their 2006 paper
#' 'Bayesian auxiliary variable models for binary and multinomial regression'
#' in Bayesian Analysis. The C++ code was written using the pseudo-code in
#' this paper as a template. Slower than \code{multilogit_hh_inv_C} but less error prone.
#' 
#'
#' @param Y An N by C numeric matrix where the ith row is a set of
#' indicators for observation i of N total observations giving which
#' of the C categories the observation is classified into.
#' @param X An N by P numeric matrix where the ith row gives
#' the values of the predictor variables for the ith outcome observation.The first column of X
#' should be an intercept column of 1s. Non-intercept X columns
#' should be centered and scaled by their standard deviations for best results. 
#' @param v a P by P numeric matrix giving the covariance matrix of coefficients.
#' The function only accepts one matrix for all categories.
#' @param n_sample positive integer giving the number of samples to draw as
#'  output after burn-in.
#' @param n_burn non-negative integer giving the number of samples of burn-in
#'  before the chain output is saved.
#' @param probs If TRUE, categorical probabilities are calculated and returned.
#' @param progress If TRUE, the function will print its progress at every 1,000th
#' iteration.
#' @family Holmes-Held methods.
#' @seealso \code{mulitlogit_holmesheld} for an R wrapper function with error
#' checking. \code{mulitlogit_hh_inv_C} for an alternative function which is faster,
#' but may have increased errors in matrix inversions.
#' @return List object containing posterior_coef, the chain of coefficient
#' values as a P by C by n_sample array. If probs are TRUE, the list will also include
#'  posterior_prob a N by C by n_sample array containing the calculated probabilities of
#'   the observation being classified into each of the C categories.
#' @examples 
#' Y <- matrix(0, nrow = 150, ncol = 3)
#' Y <- sapply(c(1,2,3), function(x) Y[, x] <- as.numeric((as.numeric(iris$Species) == x) ))
#' X <- cbind(1, iris[ , 1:4])
#' X <- as.matrix(X)
#' v <- diag(1, ncol(X))
#' out <- multilogit_holmesheld_C(Y, X, v, n_sample = 4000, n_burn = 2000)
#' 
multilogit_holmesheld_C <- function(Y, X, v, n_sample = 1000L, n_burn = 200L, probs = TRUE, progress = TRUE) {
    .Call(`_BayesMultiLogit_multilogit_holmesheld_C`, Y, X, v, n_sample, n_burn, probs, progress)
}

#' Multinomial Logistic Regression using the Polya-Gamma Method
#' 
#' @description WARNING: This function can result in R freezing in a non-interruptable state.
#' 
#' This function implements the Polya-Gamma method for
#' multinomial logistic regression. Rewritten for C++ by Jared Fisher
#' and Kyle McEvoy, but code originally written by Jesse Windle.
#' 
#' Copyright 2013 Nick Polson, James Scott, and Jesse Windle.
#' This file is part of BayesLogit, distributed under the GNU General Public
#' License version 3 or later and without ANY warranty, implied or otherwise.
#' 
#' @param Y numeric Matrix An N by C numeric matrix where the ith row is a set of
#' indicators for observation i of N giving which of the C categories the
#' observation is classified into.
#' @param X numeric Matrix An N by P numeric matrix where the ith row gives
#' the values of the predictor variables for the ith outcome observation. The first column of 
#' X should be an intercept column of 1's.
#' Non-intercept X columns should be centered and scaled by their standard deviations for best results. 
#' @param n_sample positive integer giving the number of samples to draw as
#'  output after burn-in.
#' @param n_burn non-negative integer giving the number of samples of burn-in
#'  before the chain output is saved.
#' @param progress logical If TRUE, the function will report its progress at every
#' thousandth iteration.
#' @param probs logical If TRUE, the function will calculate and return the probabilities
#' for each chain sample of each observation falling into the categories
#' based on the fitted coefficients.
#' @return List object containing posterior_coef, the chain of coefficient
#' values as an P by C by n_sample array. And, if probs is set to TRUE, 
#' posterior_prob a N by C by n_sample array containing the calculated 
#' probabilities of the observations being classified into each of the C categories.
#' 
#' @examples Y <- matrix(0, nrow = 150, ncol = 3)
#' Y <- sapply(c(1,2,3), function(x) Y[, x] <- as.numeric((as.numeric(iris$Species) == x) )) 
#' X <- scale(iris[ , 1:4])
#' X <- cbind(1, X)
#' out2 <- multilogit_PG_C(Y, X, n_sample = 2000, n_burn = 500, probs = TRUE, progress = TRUE)
#' 
multilogit_PG_C <- function(Y, X, n_sample = 1000L, n_burn = 200L, probs = TRUE, progress = TRUE) {
    .Call(`_BayesMultiLogit_multilogit_PG_C`, Y, X, n_sample, n_burn, probs, progress)
}

#' Multinomial Logistic Regression using the Holmes-Held Method
#' 
#' This function implements the Holmes-Held method for
#' multinomial logistic regression described in their 2006 paper
#' 'Bayesian auxiliary variable models for binary and multinomial regression'
#' in Bayesian Analysis. The C++ code was written using the pseudo-code in
#' this paper as a template. Faster than \code{multilogit_holmesheld_C} 
#' but more error prone.
#' 
#'
#' @param Y An N by C numeric matrix where the ith row is a set of
#' indicators for observation i of N total observations giving which
#' of the C categories the observation is classified into.
#' @param X An N by P numeric matrix where the ith row gives
#' the values of the predictor variables for the ith outcome observation. 
#' The first column of X
#' should be an intercept column of 1s. Non-intercept X columns
#' should be centered and scaled by their standard deviations for best results.
#' @param v a P by P numeric matrix giving the covariance matrix of coefficients.
#' The function only accepts one matrix for all categories.
#' @param n_sample positive integer giving the number of samples to draw as
#'  output after burn-in.
#' @param n_burn non-negative integer giving the number of samples of burn-in
#'  before the chain output is saved.
#' @param probs If set to TRUE, categorical probabilities are calculated and returned.
#' @param progress If TRUE, the function will print its progress at every 1,000th
#' iteration.
#' @family Holmes-Held methods.
#' @seealso \code{mulitlogit_holmesheld} for a wrapper function with error
#' checking. \code{mulitlogit_holmesheld_C} for an alternative function which is slower,
#' but should have fewer errors in matrix inversions.
#' @return List object containing posterior_coef, the chain of coefficient
#' values as a P by C by n_sample array. If probs are TRUE, the list will also include
#'  posterior_prob a N by C by n_sample array containing the calculated probabilities of
#'   the observation being classified into each of the C categories.
#' @examples 
#' Y <- matrix(0, nrow = 150, ncol = 3)
#' Y <- sapply(c(1,2,3), \(x) Y[, x] <- as.numeric((as.numeric(iris$Species) == x) ))
#' X <- cbind(1, iris[ , 1:4])
#' X <- as.matrix(X)
#' v <- diag(1, ncol(X))
#' out <- multilogit_hh_inv_C(Y, X, v, n_sample = 4000, n_burn = 2000)
#' 
multilogit_hh_inv_C <- function(Y, X, v, n_sample = 1000L, n_burn = 200L, probs = TRUE, progress = TRUE) {
    .Call(`_BayesMultiLogit_multilogit_hh_inv_C`, Y, X, v, n_sample, n_burn, probs, progress)
}

